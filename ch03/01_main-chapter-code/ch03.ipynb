{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bfa70ec-5c4c-40e8-b923-16f8167e3181",
   "metadata": {},
   "source": [
    "# Chương 3: Triển khai cơ chế Attention (Attention Mechanisms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e58f33e8-5dc9-4dd5-ab84-5a011fa11d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a11208-d9d3-44b1-8e0d-0c8414110b93",
   "metadata": {},
   "source": [
    "<img src=\"https://images.viblo.asia/634f6787-7c44-4939-b256-35a7bc476a38.png\" width=\"800px\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476bb600",
   "metadata": {},
   "source": [
    "Các phương pháp **attention** sẽ tìm hiểu ở phần này."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e020fd-9690-4343-80df-da96678bef5e",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/02.webp\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd1af3a",
   "metadata": {},
   "source": [
    "1. Self-attention đơn giản: Loại đơn giản nhất với các trọng số cố định => không huấn luyện được\n",
    "2. Self-attention phiên bản có trọng số có thể huấn luyện được (trainable weights)\n",
    "3. Causal attention (hay masked attention): Đảm bảo mô hình chỉ \"nhìn thấy\" các **token** trước đó trong văn bản.\n",
    "4. Multi-head attention: Phương pháp được ứng dựng thực tiễn trong các mô hình ngôn ngữ lớn, gồm nhiều bước xử lý song song.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc4dcee-34ea-4c05-9085-2f8887f70363",
   "metadata": {},
   "source": [
    "## 3.1 Vấn đề của mô hình khi xử lý một chuỗi văn bản dài"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55aa49c-36c2-48da-b1d9-70f416e46a6a",
   "metadata": {},
   "source": [
    "- Nếu dịch từng từ một theo thứ tự như ảnh dưới đây thì chắc chắn câu văn dịch ra sẽ rất rời rạc, khó hiểu.\n",
    "- Do ngữ pháp của các ngôn ngữ khác nhau, thứ tự các từ sau khi dịch có thể khác nhiều so với câu gốc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c0c433-aa4b-491e-848a-54905ebb05ad",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/03.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db03c48a-3429-48ea-9d4a-2e53b0e516b1",
   "metadata": {},
   "source": [
    "Trước khi kiến trúc **Transformer** ra đời, **RNN (Recurrent Neural Networks)** là phương pháp phổ biến nhất áp dụng cho các tác vụ dịch thuật. \n",
    "\n",
    "**RNN** là một mạng nơ-ron trong đó đầu ra từ các bước trước được đưa vào làm đầu vào cho bước hiện tại, khiến chúng rất phù hợp với dữ liệu tuần tự như văn bản. Trạng thái tại mỗi bước xử lý của RNN được gọi là **hidden state**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8df2c-c1c2-4df0-9977-ade9713088b2",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/04.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c76d6e8",
   "metadata": {},
   "source": [
    "Tuy nhiên, RNN thường gặp khó khăn với các chuỗi dài, khi chúng có thể bị \"quên\" đi các trạng thái cách xa trước đó. Các xử lý tuần tự cũng khiến cho RNN chậm hơn so với các phương pháp xử lý song song như **Transformer**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602c585-b87a-41c7-a324-c5e8298849df",
   "metadata": {},
   "source": [
    "## 3.2 Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fde64c-6034-421d-81d9-8244932086ea",
   "metadata": {},
   "source": [
    "Năm 2014, một nghiên cứu có tên **Bahdanau attention** được công bố, với nội dung chính là đề xuất một phương pháp chỉnh sửa **encoder-decoder** của RNN sao cho **decoder** có thể truy cập được vào tất cả các *token* của văn bản đầu vào. \n",
    "\n",
    "Dựa vào trọng số (attention weight) được tính toán, mô hình sẽ chọn ra đâu là từ tiếp theo phù hợp để dịch và đưa vào kết quả."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f6293-8ab5-4aeb-a04c-50ee158485b1",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/05.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2f7352",
   "metadata": {},
   "source": [
    "Hình trên mô tả bước thứ 2 trong tác vụ dịch, trọng số lớn nhất mà mô hình tính toán được ở bước này nằm ở từ **du** => Từ tiếp theo được trả về sẽ lẽ **you** (từ tương ứng của **du** trong tiếng Anh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8044be1f-e6a2-4a1f-a6dd-e325d3bad05e",
   "metadata": {},
   "source": [
    "> **Self-attention** là cơ chế giúp mô hình hiểu mối quan hệ giữa các từ trong cùng một câu, bất kể khoảng cách giữa chúng. Đây là thành phần cốt lõi của Transformer, LLMs giúp nó vượt trội so với RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6565dc9f-b1be-4c78-b503-42ccc743296c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/06.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efe05ff-b441-408e-8d66-cde4eb3397e3",
   "metadata": {},
   "source": [
    "## 3.3 Các kiểu cơ chế Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9af516-7c37-4400-ab53-34936d5495a9",
   "metadata": {},
   "source": [
    "### 3.3.1 Self-Attention đơn giản (simple self-attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8f230",
   "metadata": {},
   "source": [
    "Trọng số attention trong phương pháp này được tính trực tiếp từ dữ liệu đầu vào. Cùng xem ví dụ minh họa dưới đây."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7c7a2-b6ab-478f-ae37-faa8eaa8049a",
   "metadata": {},
   "source": [
    "<img src=\"https://images.viblo.asia/29375700-b7f7-4320-858b-3139406a5e08.png\" width=\"800px\">\n",
    "\n",
    "- Trên hình là ví dụ trong việc tính mối quan hệ giữa từ `journey` với các từ còn lại trong câu.\n",
    "- Để tính được vector `z` từ các  **token embeddings** sẽ có các **trọng số attention** `α`.\n",
    "- Để có được `α`, lại cần phải tính toán giá trị **attention score** (`ω`) trước."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a6bf3",
   "metadata": {},
   "source": [
    "### Tính Attention score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6cd5c3",
   "metadata": {},
   "source": [
    "Để tính được các giá trị **attention score** với từ `journey`, ta lấy vector $x^{(2)}$ nhân vô hướng với các vector còn lại. vector  $x^{(2)}$ trong trường hợp này sẽ được gọi với cái tên **embedded query token**.\n",
    "\n",
    "$\\omega_{2i} = x^{(2)} \\times x^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22b9556a-aaf8-4ab4-a5b4-973372b0b2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    " [[0.43, 0.15, 0.89], # Your (x^1)\n",
    " [0.55, 0.87, 0.66], # journey (x^2)\n",
    " [0.57, 0.85, 0.64], # starts (x^3)\n",
    " [0.22, 0.58, 0.33], # with (x^4)\n",
    " [0.77, 0.25, 0.10], # one (x^5)\n",
    " [0.05, 0.80, 0.55]] # step (x^6)\n",
    ")\n",
    "\n",
    "query = inputs[1]\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    " attn_scores_2[i] = torch.dot(x_i, query) # dot là phép vô hướng giữa 2 vector\n",
    "print(attn_scores_2)\n",
    "\n",
    "# Kết quả: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299baef3-b1a8-49ba-bad4-f62c8a416d83",
   "metadata": {},
   "source": [
    "Hình minh họa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb3453a-58fa-42c4-b225-86850bc856f8",
   "metadata": {},
   "source": [
    "<img src=\"https://images.viblo.asia/e429042d-9beb-428d-845f-32f41eb03c45.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be52fb-82fd-4886-a4c8-f24a9c87af22",
   "metadata": {},
   "source": [
    "### Chuẩn hóa\n",
    "\n",
    "Việc chuẩn hóa **Attention Scores** là một bước quan trọng trong cơ chế Self-Attention\n",
    "\n",
    "Tại sao lại cần chuẩn hóa **attention scores** ?\n",
    "- Không có ý nghĩa xác suất: Các giá trị Attention Scores không được chuẩn hóa không có ý nghĩa xác suất, trong khi các trọng số attention cần biểu diễn mức độ \"tập trung\" của mỗi phần tử vào các phần tử khác.\n",
    "- Không ổn định: Các giá trị **attention scores** có thể là những số rất lớn hoặc rất bé. Nếu dùng các giá trị này làm trọng số có thể xảy ra tình trạng thiên lệch khi các số lớn chi phối và làm cho các giá trị nhỏ trở nên không đáng kể.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd965d6-980c-476a-93d8-9efe603b1b3b",
   "metadata": {},
   "source": [
    "<img src=\"https://images.viblo.asia/81714ea0-9871-4040-aa4d-232efb670954.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043a567c",
   "metadata": {},
   "source": [
    "Phương pháp phổ biến để chuẩn hóa được sử dụng là hàm **softmax**, các kết quả sẽ được điều chỉnh lại sao cho tổng của tất cả chúng bằng `1`\n",
    "\n",
    "$\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$\n",
    "\n",
    "- $\\sigma(z)_i$ là xác suất của lớp i.\n",
    "- $z_i$ là điểm attention (attention score)\n",
    "- K là tổng số phần tử\n",
    "\n",
    "Ví dụ có attention scores như sau: `tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])`\n",
    "\n",
    "- $\\sum_{j=1}^{K} e^{z_j} = e^{0.9544} + e^{1.4950} + e^{1.4754} + e^{0.8434} + e^{0.7070} + e^{1.0865} = 18.7453$\n",
    "\n",
    "=> Giá trị **0.9544** sau khi chuẩn hoá: $\\frac{e^{0.9544}}{18.7453} = 0.1385$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07b2e58d-a6ed-49f0-a1cd-2463e8d53a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores: tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n",
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention scores:\", attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1cbbb-4744-41cb-8910-f5c1355555fb",
   "metadata": {},
   "source": [
    "Pytorch cung cấp sẵn hàm softmax để sử dụng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d99cac4-45ea-46b3-b3c1-e000ad16e158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e36c7-90b2-427f-94f6-bb9d31b2ab3f",
   "metadata": {},
   "source": [
    "### Tính vector context z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5943880d",
   "metadata": {},
   "source": [
    "Tính **context vector** bằng cách lấy tổng của các tích **trọng số attention** và **token embeddings**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9f5ac-8d3d-4847-94e3-fd783b7d4d3d",
   "metadata": {},
   "source": [
    "<img src=\"https://images.viblo.asia/ab263633-b9c3-48e3-be54-1234a7ffeaad.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fcb96f0-14e5-4973-a50e-79ea7c6af99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # journey\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i,x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a454262-40eb-430e-9ca4-e43fb8d6cd89",
   "metadata": {},
   "source": [
    "### 3.3.2 Tổng quát hoá"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a02bb73-fc19-4c88-b155-8314de5d63a8",
   "metadata": {},
   "source": [
    "Ở phần trên, ta chỉ đang tính cho từ thứ 2 trong câu là **journey**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6271d4d7",
   "metadata": {},
   "source": [
    "Để tính toàn bộ giá trị của các vector z, ta thực hiện lần lượt các bước như sau:\n",
    "\n",
    "- 1. Tính **attention scores** bằng cách lấy ma trận **token embeddings** nhân cho ma trận chuyển vị (transpose) của nó.\n",
    "- 2. Chuẩn hóa **attention scores** thành **trọng số attention**\n",
    "- 3. Tính **context vector z** bằng cách lấy  **trọng số attention** nhân cho **token embeddings**\n",
    "​\n",
    "\n",
    "**Lưu ý**: Việc tính **attention score** mà chúng ta đã thực hiện bản chất là nhân dòng thứ 2 của ma trận `inputs` lần lượt với các cột của ma trận `inputs^T` (ma trận chuyển vị của inputs).\n",
    "\n",
    "\n",
    "$$ inputs = \n",
    "\\begin{bmatrix}\n",
    "0.43 & 0.15 & 0.89 \\\\\n",
    "0.55 & 0.87 & 0.66 \\\\\n",
    "0.57 & 0.85 & 0.64 \\\\\n",
    "0.22 & 0.58 & 0.33 \\\\\n",
    "0.77 & 0.25 & 0.10 \\\\\n",
    "0.05 & 0.80 & 0.55\n",
    "\\end{bmatrix},\n",
    "inputs^T = \n",
    "\\begin{bmatrix}\n",
    "0.43 & 0.55 & 0.57 & 0.22 & 0.77 & 0.05 \\\\\n",
    "0.15 & 0.87 & 0.85 & 0.58 & 0.25 & 0.80 \\\\\n",
    "0.89 & 0.66 & 0.64 & 0.33 & 0.10 & 0.55\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bffe4b-56fe-4c37-9762-24bd924b7d3c",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/12.webp\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa652506-f2c8-473c-a905-85c389c842cc",
   "metadata": {},
   "source": [
    "- Bước 1: Ma trận 6x3 nhân với ma trận 3x6 cho ra ma trận có kích thước 6x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04004be8-07a1-468b-ab33-32e16a551b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1539187f-1ece-47b7-bc9b-65a97115f1d4",
   "metadata": {},
   "source": [
    "- Ngắn gọn hơn, ta có"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2cea69d0-9a47-45da-8d5a-47ceef2df673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4bac4-acfd-427f-9b11-c436ac71748d",
   "metadata": {},
   "source": [
    "- Bước 2: Chuẩn hoá với Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa4ef062-de81-47ee-8415-bfe1708c81b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa6d02b-7f15-4eb4-83a7-0b8a819e7a0c",
   "metadata": {},
   "source": [
    "- Kiểm tra nhanh xem các giá trị ở dòng thứ 2 có tổng bằng 1 không ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "112b492c-fb6f-4e6d-8df5-518ae83363d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2 sum: 1.0\n",
      "All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "row_2_sum = sum([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
    "print(\"Row 2 sum:\", row_2_sum)\n",
    "\n",
    "print(\"All row sums:\", attn_weights.sum(dim=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b0b5c-d813-44c7-b373-fde9540ddfd1",
   "metadata": {},
   "source": [
    "- Bước 3: Tính vector z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba8eafcf-f7f7-4989-b8dc-61b50c4f81dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b245b8-7732-4fab-aa1c-e3d333195605",
   "metadata": {},
   "source": [
    "- $z^{(2)} = [0.4419, 0.6515, 0.5683]$ cũng y chang kết quả tính ở phần trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2570eb7d-aee1-457a-a61e-7544478219fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "print(\"Previous 2nd context vector:\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a303b6fb-9f7e-42bb-9fdb-2adabf0a6525",
   "metadata": {},
   "source": [
    "## 3.4 Self-Attention với trọng số có thể huấn luyện được (self-attention with trainable weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88363117-93d8-41fb-8240-f7cfe08b14a3",
   "metadata": {},
   "source": [
    "- Với trường hợp self-attention đơn giản, các trọng số được tính toán theo một công thức toán học nhất định => không thể điều chỉnh hay thay đổi.\n",
    "- Trọng số có thể huấn luyện được là trọng số có thể thay đổi, tối ưu hoá."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9492ba-6f66-4f65-bd1d-87cf16d59928",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/13.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90a77e-d746-4704-9354-1ddad86e6298",
   "metadata": {},
   "source": [
    "### 3.4.1 Tính attention weights từng bước một"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e95a46-1f67-4b71-9e84-8e2db84ab036",
   "metadata": {},
   "source": [
    "- Ta có **$W_Q$, $W_K$, $W_V$** lần lượt là ba ma trận.\n",
    "- 3 ma trận sẽ được sử dụng trong quá trình tính toán trọng số attention.\n",
    "- Thông số trong các ma trận có thể được điều chỉnh qua quá trình huấn luyện."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d996671-87aa-45c9-b2e0-07a7bcc9060a",
   "metadata": {},
   "source": [
    "- Gọi q, k, v lần lượt là các vector query, key, value\n",
    "\n",
    "  - Query vector: $q^{(i)} = x^{(i)}\\,W_q $\n",
    "  - Key vector: $k^{(i)} = x^{(i)}\\,W_k $\n",
    "  - Value vector: $v^{(i)} = x^{(i)}\\,W_v $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f334313-5fd0-477b-8728-04080a427049",
   "metadata": {},
   "source": [
    "- Kích thước vector $x$ và vector $q$ có thể giống hoặc khác nhau, tùy thuộc vào thiết kế và cách triển khai cụ thể của mô hình.\n",
    "- Trong các mô hình GPT, kích thước đầu vào và đầu ra thường giống nhau, nhưng để minh họa và dễ theo dõi các phép tính, chúng ta chọn kích thước đầu vào và đầu ra khác nhau ở đây:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f35c870",
   "metadata": {},
   "source": [
    "<img src=\"https://images.viblo.asia/c2255234-587d-4f21-b158-dc9de24514d6.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8250fdc6-6cd6-4c5b-b9c0-8c643aadb7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1] # Từ journey\n",
    "d_in = inputs.shape[1] # kích thước đầu vào d=3\n",
    "d_out = 2 # kích thước đầu ra d=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfd7259a-f26c-4cea-b8fc-282b5cae1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# requires_grad=False là cài đặt để các ma trận sẽ không cập nhật trong quá trình huấn luyện\n",
    "# Giữ cố định để chúng ta dễ theo dõi kết quả\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key   = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfd0b50-7701-4adb-821c-e5433622d9c4",
   "metadata": {},
   "source": [
    "- Tính các vector query, key và value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73cedd62-01e1-4196-a575-baecc6095601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c1c3949-fc08-4d19-a41e-1c235b4e631b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac5dfd6-ade8-4e7b-b0c1-bed40aa24481",
   "metadata": {},
   "source": [
    "- Bước tiếp theo, **Bước 2**: Tính attention score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b9426d",
   "metadata": {},
   "source": [
    "$ω_{2i} = q^{(2)} \\times k^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0a2b7-5c50-4ede-90cf-7ad74412b3aa",
   "metadata": {},
   "source": [
    "<img src=\"https://images.viblo.asia/b9318798-52d2-493f-97ad-67b4af02340d.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64cbc253-a182-4490-a765-246979ea0a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "keys_2 = keys[1] # Python starts index at 0\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22) # omega_22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d15c0-c24e-4e6f-a160-6349b418f935",
   "metadata": {},
   "source": [
    "- Tính cả 6 giá trị attention score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b14e44b5-d170-40f9-8847-8990804af26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # chuyển vị của keys\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1609edb-f089-461a-8de2-c20c1bb29836",
   "metadata": {},
   "source": [
    "- Tiếp theo, **Bước 3**, tính **attention weights** bằng cách chuẩn hoá **attention scores**.\n",
    "- Softmax trong trường hợp này là lấy **attention scores** chia cho  $\\sqrt{d_k}$ Với $d_k$ là kích thước vector keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "146f5587-c845-4e30-9894-c7ed3a248153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d_k: 2\n",
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "print(\"d_k:\", d_k)\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f61a28-b103-434a-aee1-ae7cbd821126",
   "metadata": {},
   "source": [
    "<img src=\"https://images.viblo.asia/69eff0fc-a3ff-42a2-8453-d86d09f3ebb2.png\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1890e3f9-db86-4ab8-9f3b-53113504a61f",
   "metadata": {},
   "source": [
    "- Tại **Bước 4**, Tính context vector của từ `journey`:\n",
    "\n",
    " $z^{2} = α_{(2)} \\times v$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e138f033-fa7e-4e3a-8764-b53a96b26397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b2907-e448-473e-b46c-77735a7281d8",
   "metadata": {},
   "source": [
    "### 3.4.2 Công thức tổng quát"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04313410-3155-4d90-a7a3-2f3386e73677",
   "metadata": {},
   "source": [
    "Sau khi đã tính được context vector **$z_2$**.Chúng ta cùng xem các bước tổng quát để tính cả **context vector z**\n",
    "\n",
    ">Bước 1.  Với **X** là vector input embeddings và $b_q, b_k, b_v$ là các hệ số tự do (bias)\n",
    ">\n",
    ">$Q = X \\cdot W_Q + b_q$, \n",
    ">\n",
    ">$K = X \\cdot W_K + b_k$, \n",
    ">\n",
    ">$V = X \\cdot W_V + b_v$\n",
    ">\n",
    "> Bước 2. $Attention\\ Scores=Q⋅K^{T}$\n",
    "> \n",
    "> Bước 3: $Attention\\ Weights = softmax(\\frac{QK^T}{\\sqrt{d_k}})$ với $d_k$ là số chiều của k (ví dụ trên thì k = 2)\n",
    "> \n",
    ">  Bước 4: $z = Attention Weights⋅V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "51590326-cdbe-4e62-93b1-17df71c11ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee1a024-84a5-425a-9567-54ab4e4ed445",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/18.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048e0c16-d911-4ec8-b0bc-45ceec75c081",
   "metadata": {},
   "source": [
    "- Chúng ta có sử dụng `Linear` của **PyTorch** thay thế cho `Parameter`\n",
    "\n",
    "Một số lợi thế lớn khác của việc sử dụng `nn.Linear` so với `nn.Parameter(torch.rand(...)`:\n",
    "- Có hỗ trợ bias (không dùng thì cho bằng False).\n",
    "- `nn.Linear` sử dụng phương pháp khởi tạo trọng số được tối ưu hóa giúp mô hình đạt kết quả huấn luyện nhanh hơn. Trong khi đó, `torch.rand(...)` chỉ tạo ngẫu nhiên trong khoảng [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73f411e3-e231-464a-89fe-0a9035e5f839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5025b37-0f2c-4a67-a7cb-1286af7026ab",
   "metadata": {},
   "source": [
    "## 3.Causal attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef0a6b8-205a-45bf-9d26-8fd77a8a03c3",
   "metadata": {},
   "source": [
    "**Causal Attention** (hay còn gọi là Masked Attention) là một kỹ thuật được nhằm che dấu các thông tin phía sau khiến mô hình chỉ nắm được các dữ liệu ở phía trước.\n",
    "\n",
    "Nói đơn giản hơn, điều này đảm bảo rằng việc dự đoán từ tiếp theo chỉ nên phụ thuộc vào các từ đứng trước nó."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e91bb5-5aae-4f05-8a95-973b3f988a35",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/19.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f405de-cd86-4e72-8f3c-9ea0354946ba",
   "metadata": {},
   "source": [
    "### 3.5.1 Triển khai Causal attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f28d0-8218-48e4-8b9c-bdc5ce489218",
   "metadata": {},
   "source": [
    "- Chuẩn hoá **attention scores** để thu được **attention weights**\n",
    "- Áp dụng Causal attention thu được **Masked attention scores**\n",
    "- Chuẩn hoá một lần nữa để thu được **Masked attention weights**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f99af3-32bc-48f5-8eb4-63504670ca0a",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/20.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351d5977",
   "metadata": {},
   "source": [
    "**Bước 1**: Chuẩn hoá attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1933940d-0fa5-4b17-a3ce-388e5314a1bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# attention_scores => attention_weights\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89020a96-b34d-41f8-9349-98c3e23fd5d6",
   "metadata": {},
   "source": [
    "**Bước 2**: Áp dụng casual attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43f3d2e3-185b-4184-9f98-edde5e6df746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Tạo 1 ma trận tam giác dưới có kích thước bằng ma trận attention_weights ở trên\n",
    "context_length = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce2b08-3583-44da-b3fc-cabdd38761f6",
   "metadata": {},
   "source": [
    "- Nhân 2 ma trận `mask_simple` và `attention_weights` với nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f531e2e-f4d2-4fea-a87f-4c132e48b9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple = attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db92d7-c397-4e42-bd8a-6a2b3e237e0f",
   "metadata": {},
   "source": [
    "**Bước 3**: Chuẩn hoá mask_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d392083-fd81-4f70-9bdf-8db985e673d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512e7cf4-dc0e-4cec-948e-c7a3c4eb6877",
   "metadata": {},
   "source": [
    "Cách trên phải áp dụng softmax tận 2 lần, ta cùng đến với cách thứ 2 ngắn gọn hơn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb682900-8df2-4767-946c-a82bee260188",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/21.webp\" width=\"450px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd67411",
   "metadata": {},
   "source": [
    "- Bước 1: Thay thay các giá trị nằm trên đường chéo chính của ma trận **attention scores** thành `-inf`\n",
    "- Bước 2: Chuẩn hóa với softmax cho ra **trọng số attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a2be2f43-9cf0-44f6-8d8b-68ef2fb3cc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5f803-d735-4543-b9da-00ac10fb9c50",
   "metadata": {},
   "source": [
    "So với cách 1, cách này sử dụng ít bước hơn. Khác biệt cơ bản giữa 2 cách là việc thay thế các giá trị trên đường chéo chính thành `-inf`.\n",
    "\n",
    "Với giá trị `-inf`, $-inf = -\\infty \\implies e^{-\\infty} = 0 \\rightarrow \\text{xác suất tương ứng} = 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1cd6d7f-16f2-43c1-915e-0824f1a4bc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636fc5f-6bc6-461e-ac6a-99ec8e3c0912",
   "metadata": {},
   "source": [
    "### 3.5.2 Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3dc7ee-6539-4fab-804a-8f31a890c85a",
   "metadata": {},
   "source": [
    "- Ngoài việc che giấu thông tin bằng **casual attention**. Trong thực tế, người ta còn áp dụng thêm kỹ thuật **Dropout** nhằm loại bỏ ngẫu nhiên một phần tham số của mô hình. \n",
    "\n",
    "- Cần lưu ý rằng dropout chỉ được sử dụng trong quá trình huấn luyện.\n",
    "\n",
    "Hình dưới đây minh họa kỹ thuật **dropout** với tỷ lệ 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee799cf6-6175-45f2-827e-c174afedb722",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/22.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a575458-a6da-4e54-8688-83e155f2de06",
   "metadata": {},
   "source": [
    "- Khi áp dụng dropout, các giá trị không bị loại bỏ sẽ được bù lại cách nhân với một hệ số có giá trị `1 / (1 - dropout_rate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0de578db-8289-41d6-b377-ef645751e33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n",
      "tensor([[2., 2., 0., 2., 2., 0.],\n",
      "        [0., 0., 0., 2., 0., 2.],\n",
      "        [2., 2., 2., 2., 0., 2.],\n",
      "        [0., 2., 2., 0., 0., 2.],\n",
      "        [0., 2., 0., 2., 0., 2.],\n",
      "        [0., 2., 2., 2., 2., 0.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) # dropout 50%\n",
    "example = torch.ones(6, 6) # Ma trận đơn vị 6x6 với các phần tử là 1\n",
    "print(example)\n",
    "print(dropout(example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b16c5edb-942b-458c-8e95-25e4e355381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc14639-5f0f-4840-aa9d-8eb36ea90fb7",
   "metadata": {},
   "source": [
    "### 3.5.3 Tổng hợp lại với causal self-attention class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c41d29-1933-43dc-ada6-2dbb56287204",
   "metadata": {},
   "source": [
    "Viết class `CausalAttention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "977a5fa7-a9d5-4e2e-8a32-8e0331ccfe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60d8c2eb-2d8e-4d2c-99bc-9eef8cc53ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Triển khai causal attention, sau đó áp dụng dropout\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2)\n",
    "        attn_scores.masked_fill_(\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a554cf47-558c-4f45-84cd-bf9b839a8d50",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/23.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bef90f-cfd4-4289-b0e8-6a00dc9be44c",
   "metadata": {},
   "source": [
    "## 3.6 Từ single-head attention đến multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15f0cfc",
   "metadata": {},
   "source": [
    "Sự khác biệt cơ bản giữa **single-head attention** và **multi-head attention** là ở số lượng ma trận **$W_Q$, $W_K$, $W_V$**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11697757-9198-4a1c-9cee-f450d8bbd3b9",
   "metadata": {},
   "source": [
    "### 3.6.1 Multi-head bằng cách xếp chồng nhiều lớp single-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70766faf-cd53-41d9-8a17-f1b229756a5a",
   "metadata": {},
   "source": [
    "- Dưới đây là hình minh họa cho cấu trúc 2 **single-head attention**\n",
    "\n",
    "<img src=\"https://images.viblo.asia/950e0003-9047-435c-b239-c6edc30ada5d.png\" width=\"800px\">\n",
    "\n",
    "- Chúng ta chỉ cần xếp chồng nhiều single-head attention để tạo thành một multi-head attention:\n",
    "\n",
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/25.webp\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b274b0",
   "metadata": {},
   "source": [
    "- Mỗi head tính toán song song với nhau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9a66e11-7105-4bb4-be84-041f1a1f3bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    # num_heads: head attention\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193d3d2b-2578-40ba-b791-ea2d49328e48",
   "metadata": {},
   "source": [
    "- d_out = 2: kích thước của mỗi vector key, query, value trong mỗi đầu attention (attention head).\n",
    "- Số attention head = 2\n",
    "\n",
    "=> Mỗi head sẽ tạo ra một context vector có kích thước `d_out = 2`. Vì có 2 head nên vector Z cuối cùng thu được có 2x2 = **4 chiều**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6836b5da-ef82-4b4c-bda1-72a462e48d4e",
   "metadata": {},
   "source": [
    "### 3.6.2 Triển khai multi-head attention with weight splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b48d0d-71ba-4fa0-b714-ca80cabcb6f7",
   "metadata": {},
   "source": [
    "\n",
    "- Thay vì nhiều $W_{query}, W_{key}, W_{value}$ như ở trên, chúng ta tạo ra các 1 bộ ma trận duy nhất rồi sau đó chia chúng thành các ma trận riêng lẻ cho từng head attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1134595b",
   "metadata": {},
   "source": [
    "Hình dưới minh hoạ sự khác nhau giữa việc có **weight splits** và không **weight splits**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c02d96",
   "metadata": {},
   "source": [
    "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/26.webp\" width=\"800px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9734dbd7",
   "metadata": {},
   "source": [
    "Ưu điểm của việc **weight splits**\n",
    "- Tối ưu hóa tốt hơn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b0188-6e9e-4e56-a988-10523c6c8538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        # Đảm bảo d_out chia hết cho số lượng heads\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Kích thước cho mỗi head\n",
    "\n",
    "        # Các ma trận W\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # Lớp để gộp kết quả từ các head lại\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # causal mask\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape  # batch size, số token, kích thước đầu vào\n",
    "\n",
    "        # Biến đổi input thành các tensor query, key, value\n",
    "        keys = self.W_key(x)      # Kích thước (shape): (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Chia nhỏ thành các head: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        print(\"Split Keys: \", keys)\n",
    "\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Đưa chiều num_heads lên trước: -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        print(\"Transpose Keys: \", keys)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Tính attention scores bằng cách nhân Q với K^T\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # (b, num_heads, num_tokens, num_tokens)\n",
    "\n",
    "        # Áp dụng causal attention\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)  # Gán -inf vào các vị trí bị che\n",
    "\n",
    "        # Chuẩn hóa attention scores bằng softmax\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        # Áp dụng dropout\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Tính toán context vector: (b, num_heads, num_tokens, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)  # Đổi lại về (b, num_tokens, num_heads, head_dim)\n",
    "\n",
    "        # Gộp các head lại: (b, num_tokens, num_heads * head_dim = d_out)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "\n",
    "        # Dự đoán đầu ra cuối cùng (có thể học được)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a159e17f",
   "metadata": {},
   "source": [
    "Ví dụ cụ thể  về cách chia và biến đổi các head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c938f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    " [[1,2,3,4,5,6],   [7,8,9,10,11,12],   [13,14,15,16,17,18]],\n",
    " [[19,20,21,22,23,24],   [25,26,27,28,29,30],   [31,32,33,34,35,36]]\n",
    "]\n",
    "\n",
    "# Chuyển sang Tensor\n",
    "x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "# Batch 0 có 3 token\n",
    "# Batch 1 cũng có 3 token\n",
    "# Mỗi token là 1 vector 6 chiều.\n",
    "\n",
    "b, num_tokens, d_in = x.shape # 2, 3, 6\n",
    "num_heads = 2\n",
    "d_out = 6\n",
    "head_dim = d_out // num_heads # 3\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "81a1129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attention = MultiHeadAttention(d_in, d_out, context_length, dropout, num_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1c1c4c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Keys:  tensor([[[[ -1.2433,  -4.1203,   0.3265],\n",
      "          [ -0.9697,   5.6101,  -5.8388]],\n",
      "\n",
      "         [[ -4.0821,  -7.7870,   0.9813],\n",
      "          [ -2.2691,  14.9648, -15.1872]],\n",
      "\n",
      "         [[ -6.9209, -11.4537,   1.6360],\n",
      "          [ -3.5685,  24.3196, -24.5356]]],\n",
      "\n",
      "\n",
      "        [[[ -9.7597, -15.1204,   2.2908],\n",
      "          [ -4.8678,  33.6743, -33.8840]],\n",
      "\n",
      "         [[-12.5985, -18.7871,   2.9455],\n",
      "          [ -6.1672,  43.0290, -43.2324]],\n",
      "\n",
      "         [[-15.4373, -22.4538,   3.6003],\n",
      "          [ -7.4666,  52.3837, -52.5808]]]], grad_fn=<ViewBackward0>)\n",
      "Transpose Keys:  tensor([[[[ -1.2433,  -4.1203,   0.3265],\n",
      "          [ -4.0821,  -7.7870,   0.9813],\n",
      "          [ -6.9209, -11.4537,   1.6360]],\n",
      "\n",
      "         [[ -0.9697,   5.6101,  -5.8388],\n",
      "          [ -2.2691,  14.9648, -15.1872],\n",
      "          [ -3.5685,  24.3196, -24.5356]]],\n",
      "\n",
      "\n",
      "        [[[ -9.7597, -15.1204,   2.2908],\n",
      "          [-12.5985, -18.7871,   2.9455],\n",
      "          [-15.4373, -22.4538,   3.6003]],\n",
      "\n",
      "         [[ -4.8678,  33.6743, -33.8840],\n",
      "          [ -6.1672,  43.0290, -43.2324],\n",
      "          [ -7.4666,  52.3837, -52.5808]]]], grad_fn=<TransposeBackward0>)\n",
      "Attention scores:  tensor([[[[   11.7901,    27.4211,    43.0520],\n",
      "          [   32.2474,    73.3888,   114.5302],\n",
      "          [   52.7047,   119.3566,   186.0084]],\n",
      "\n",
      "         [[  -45.7824,  -120.1584,  -194.5344],\n",
      "          [ -103.2509,  -271.6193,  -439.9877],\n",
      "          [ -160.7195,  -423.0802,  -685.4409]]],\n",
      "\n",
      "\n",
      "        [[[  349.6490,   441.8113,   533.9736],\n",
      "          [  446.6377,   564.3105,   681.9833],\n",
      "          [  543.6263,   686.8096,   829.9927]],\n",
      "\n",
      "         [[-1287.2473, -1643.6006, -1999.9536],\n",
      "          [-1626.6931, -2077.0388, -2527.3843],\n",
      "          [-1966.1388, -2510.4766, -3054.8145]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = multihead_attention(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f2f191",
   "metadata": {},
   "source": [
    "Split Keys:\n",
    "```python\n",
    "tensor([\n",
    "      [ # batch 0\n",
    "        [[ -1.4421,   0.5373,   0.5513],[ -1.6091,  -0.8023,   0.4414]], # token 0 -> head 0: [-1.4421,   0.5373,   0.5513], head 1: [1.6091,  -0.8023,   0.4414]\n",
    "        [[ -3.8594,   3.2438,   1.5168], [ -5.4921,  -0.9958,   3.2902]],# token1\n",
    "        [[ -6.2767,   5.9502,   2.4822],[ -9.3751,  -1.1893,   6.1390]]  # token2\n",
    "      ],\n",
    "      [ # batch 1\n",
    "        [[ -8.6940,   8.6567,   3.4477], [-13.2580,  -1.3828,   8.9877]], # token3\n",
    "        [[-11.1113,  11.3632,   4.4131], [-17.1410,  -1.5763,  11.8365]], # token4\n",
    "        [[-13.5286,  14.0696,   5.3785], [-21.0240,  -1.7698,  14.6853]]  # token5\n",
    "      ]\n",
    "    ], grad_fn=<ViewBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fa16e",
   "metadata": {},
   "source": [
    "Transpose Keys: Biến đổi lại để mỗi head xử lý các tokens của riêng nó.\n",
    "```python\n",
    "tensor([[\n",
    "        # Các token do head0 xử lý\n",
    "        [\n",
    "            [ -1.4421,   0.5373,   0.5513],\n",
    "            [ -3.8594,   3.2438,   1.5168],\n",
    "            [ -6.2767,   5.9502,   2.4822]\n",
    "        ],\n",
    "\n",
    "        # Các token do head1 xử lý\n",
    "        [\n",
    "            [ -1.6091,  -0.8023,   0.4414],\n",
    "            [ -5.4921,  -0.9958,   3.2902],\n",
    "            [ -9.3751,  -1.1893,   6.1390]\n",
    "        ]],\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb582daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 3, 6])\n",
      "Output:\n",
      " tensor([[[  0.1784,   0.0321,   1.4047,  -1.5837,   1.2440,  -1.6398],\n",
      "         [  0.0717,   0.1074,   1.4796,  -1.8049,   1.2127,  -1.7823],\n",
      "         [  0.1723,   0.0364,   1.4089,  -1.5963,   1.2422,  -1.6479]],\n",
      "\n",
      "        [[  1.0399,   5.8948,   4.3728, -16.1517,   1.6774,  -8.0966],\n",
      "         [  1.0399,   5.8948,   4.3728, -16.1518,   1.6774,  -8.0966],\n",
      "         [  1.0399,   5.8948,   4.3728, -16.1517,   1.6774,  -8.0966]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# In kết quả cuối cùng\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output:\\n\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
