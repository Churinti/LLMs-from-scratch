# 第二章 处理文本数据
## 2.1 理解词嵌入
- 词嵌入是将离散的词元映射到连续的向量空间
- 词嵌入的目标是将语义相似的词元映射到相近的向量
- 词嵌入的好处是可以捕捉到词元之间的语义关系
- 词嵌入的应用包括文本分类、情感分析、机器翻译等
- 词嵌入的实现方法包括Word2Vec等,~~以及OPENAI的tiktoken，本文中用了GPT的Tiktoken~~ (注：tiktoken是分词工具，不是嵌入方法)

## 2.2 文本分词
- 文本分词是将文本划分为一个个词元的过程
- 文本分词的目标是将文本转换为一个个离散的词元
- 文本分词的好处是可以将文本转换为计算机可以处理的格式
- 有一些常用的分词工具和库
    - NLTK
    - SpaCy
    - Jieba 中文分词，在练习中有使用
    - Tiktoken GPT的分词工具，在练习中有使用,属于BPE的一种

## 2.3 词元转换为tokenID
- 词元转换为tokenID是将离散的词元映射到连续的整数ID的过程
- 词元转换为tokenID的目标是将词元转换为计算机可以处理的格式
- 词元转换为tokenID的好处是可以将词元映射到一个稠密的向量空间
- 词元转换为tokenID的实现方法包括使用词表和哈希表等
- GPT的tiktoken使用BPE算法来构建词表,4o 使用了200k 的词表
>[https://github.com/openai/tiktoken](https://github.com/openai/tiktoken)

## 2.4 引入特殊token
- 特殊token是一些特殊的词元，用于表示一些特殊的含义
- 特殊token的目标是将一些特殊的含义表示出来
- 特殊token的好处是可以将一些特殊的含义表示出来
- 特殊token的实现方法包括使用一些特殊的字符或字符串
- 常用的特殊token包括
    - [PAD]：填充token，用于填充序列到相同长度
    - [UNK]：未知token，用于表示词表中不存在的词元
    - [CLS]：分类token，用于表示序列的开始
    - [SEP]：分隔token，用于表示序列的结束
    - [MASK]：掩码token，用于表示需要被掩码的词元
- 这些特殊token的使用可以帮助模型更好地理解文本的结构和语义
- 例如，在文本分类任务中，可以使用[CLS] token来表示序列的开始，并在模型的输出中使用该token的嵌入向量来进行分类
- 在序列标注任务中，可以使用[MASK] token来表示需要被掩码的词元，并在模型的训练过程中使用该token来进行掩码语言模型的训练
- 在文本生成任务中，可以使用[SEP] token来表示序列的结束，并在模型的输出中使用该token来进行文本的生成
- 在文本填充任务中，可以使用[PAD] token来表示填充的词元，并在模型的训练过程中使用该token来进行填充
- 在文本去噪任务中，可以使用[UNK] token来表示未知的词元，并在模型的训练过程中使用该token来进行去噪
- 在文本分割任务中，可以使用[SEP] token来表示序列的分隔，并在模型的训练过程中使用该token来进行分割

## 2.5 BPE分词
- BPE分词是一种基于字节对编码的分词方法
- BPE分词的目标是将文本划分为一个个词元
- BPE分词的好处是可以将文本划分为一个个词元
- BPE分词的实现方法包括使用字节对编码算法
- BPE分词的步骤包括
    - 统计文本中所有的字节对
    - 将字节对替换为一个新的词元
    - 重复以上步骤，直到达到预设的词表大小
- BPE分词的优点是可以处理未登录词和低频词
- BPE分词的缺点是需要预先设定词表大小 (注：tiktoken的200k 的大小为199999)

## 2.6 使用滑动窗口进行数据采样
- 训练数据集的构建(以下为gpt生成)
    - 训练数据集是一个包含多个样本的集合
    - 每个样本由输入序列和目标序列组成
    - 输入序列是一个包含多个词元的序列
    - 目标序列是一个包含多个词元的序列
    - 输入序列和目标序列的长度可以不同
    - 输入序列和目标序列的词元可以相同，也可以不同
    - 输入序列和目标序列的词元可以重复，也可以不重复
    - 输入序列和目标序列的词元可以是任意类型的词元
    - 输入序列和目标序列的词元可以是任意长度的词元
    - 输入序列和目标序列的词元可以是任意数量的词元

- 训练数据集的构建方法
    - 使用滑动窗口的方法来构建训练数据集
    - 滑动窗口的大小可以根据需要进行调整
    - 滑动窗口的步长可以根据需要进行调整
    - 滑动窗口的大小和步长可以影响训练数据集的质量和数量
    - 滑动窗口的大小和步长可以影响模型的训练效果和性能
    - 滑动窗口的大小和步长可以影响模型的训练时间和资源消耗
    - 滑动窗口的大小和步长可以影响模型的训练速度和收敛速度
    - 滑动窗口的大小和步长可以影响模型的训练精度和泛化能力
    - 滑动窗口的大小和步长可以影响模型的训练稳定性和鲁棒性
    - 滑动窗口的大小和步长可以影响模型的训练效率和效果
    - 滑动窗口的大小和步长可以影响模型的训练质量和数量

- 训练数据集的构建示例
    - 假设我们有一个文本序列"我爱自然语言处理"
    - 我们可以使用滑动窗口的方法来构建训练数据集
    - 假设滑动窗口的大小为3，步长为1
    - 那么我们可以得到以下训练数据集
        - 输入序列：["我", "爱", "自然"]
        - 目标序列：["爱", "自然", "语言"]
        - 输入序列：["爱", "自然", "语言"]
        - 目标序列：["自然", "语言", "处理"]
        - 输入序列：["自然", "语言", "处理"]
        - 目标序列：["语言", "处理", "结束"]
    - 这样我们就得到了一个包含多个样本的训练数据集
    - 训练数据集的大小为3
    - 训练数据集的样本数为3
    - 训练数据集的样本长度为3
    - 训练数据集的样本词元数为3
    - 训练数据集的样本词元长度为3
    - 训练数据集的样本词元类型为字符串
    - 训练数据集的样本词元数量为3
    - 训练数据集的样本词元大小为3
    - 训练数据集的样本词元形状为(3, 3)
    - 训练数据集的样本词元维度为3
    - 训练数据集的样本词元维度大小为3
    - 训练数据集的样本词元维度形状为(3, 3)
    - 训练数据集的样本词元维度类型为整数
    - 训练数据集的样本词元维度数量为3

-  **我的读书记录**
    - Pytorch的Dataset类，提取数据，每行包含的词元的大小由`max_lenght`决定
    - `max_lenght`的大小决定了每个样本的大小
    - 返回的数据结构为iterable的对象
    - `__next__`方法返回一个字典，包含了两个张量，输入序列和目标序列
    ```python
    dataloaders = DataLoader(dataset, batch_size=1, max_length=4, stride=1,shuffle=False)
    dataiter = iter(dataloaders)
    first_batch = next(dataiter)
    print(first_batch)
    ```
    ```python
    [tensor([[165508,  23022,    198,    271]])
    ,tensor([[ 23022,    198,    271,  32720]])]

    ```


## 2.7 词嵌入
- 嵌入矩阵 (注：简单实现用没有包含可学习的权重矩阵，假设是一张固定的嵌入矩阵，每一行是一个词元的嵌入向量)
    - 词元只是一个整数ID
    - ***词元的嵌入矩阵是一个二维矩阵，行数为词表大小，列数为嵌入维度***
    - 每一行对应一个词元的嵌入向量
    - 词元的嵌入向量是一个高维稠密向量
    > 以下内容在本节中还没有体现
    - ~~嵌入矩阵的权重是可学习的参数~~
    - ~~嵌入矩阵的权重在训练过程中会不断更新~~
    - ~~嵌入矩阵的权重可以通过随机初始化或预训练模型加载~~
- 嵌入层的权重矩阵由小的随机值构成
	- 嵌入层的实质就是执行一种查找操作
    - ***对应的是每个token在嵌入矩阵中的行索引***
- 每个词元可以得到在当前上下文中，所有词元的注意力权重，shape为[1,n] n= input token number

## 2.8 位置编码
- 位置编码是将词元的位置信息编码到嵌入向量中的方法
- 位置编码的目标是将词元的位置信息表示出来
- 位置编码的好处是可以将词元的位置信息表示出来
- 位置编码的实现方法包括使用正弦和余弦函数
- 位置编码的步骤包括
    - 将词元的位置信息转换为一个高维稠密向量
    - 将词元的位置信息添加到嵌入向量中
    - 将嵌入向量和位置编码向量相加
- 位置编码的优点是可以处理未登录词和低频词
- 位置编码的缺点是需要预先设定位置编码的维度
- 位置编码的实现方法包括使用正弦和余弦函数
- OPENAI的GPT模型使用了一种基于正弦和余弦函数的位置编码方法，绝对位置编码

    ### 创建位置编码
    - 假设batch_size为8，序列长度为4，嵌入维度为256
    - 词元的张量形状为[8, 4]
    - 嵌入编码的张量形状为[8, 4, 256]

    ### 采用绝对位置编码嵌入
    - 位置编码的张量形状为[4, 256]
    ```python
    print(embeddings.shape) 
    torch.Size([4, 256])
    ## 位置嵌入张量由4个256维的向量组成
    ```
    ### pytorch 会在每个批次中的每个嵌入张量添加位置编码，
    ``` python
    input_embeddings = token_embeddings + position_embeddings
    # 位置编码的张量形状为[4, 256]   
    # 词元嵌入编码的张量形状为[8, 4, 256]
    # 两个张量相加后，得到的张量形状为[8, 4, 256]，即每次相加，位置编码会复制自己
    print(input_embeddings[0].shape)
